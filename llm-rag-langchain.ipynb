{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b78d9b-5083-44bb-b3e3-613dde613b40",
   "metadata": {},
   "source": [
    "# <span style=\"color: #2E86C1; font-weight: bold; font-size: 36px;\">Smarter Food Choices with RAG using OpenVINO</span>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## <span style=\"color: #2980B9; font-weight: bold;\">Project Overview</span>\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG) Assistant for Nutrition**\n",
    "\n",
    "This project leverages the capabilities of **LLaMA 3.1 8B Instruct**, optimized with the **Intel OpenVINO toolkit**, creating an assistant that empowers users to make informed dietary choices through image recognition and real-time data generation.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color: #2874A6; font-weight: bold;\">Key Features and Technologies</span>\n",
    "\n",
    "#### **1. Intel AI PC**  \n",
    "- **Purpose and Role**: The Intel AI PC is a high-performance device built for AI and machine learning workloads, featuring a unique blend of CPUs, GPUs, and AI accelerators within Intel’s XPU architecture.\n",
    "- **Benefits for the Project**: With Intel AI PC, the assistant runs complex deep learning models and real-time inference, ensuring smooth, immediate responses.\n",
    "- **Why It Matters**: The Intel AI PC handles demanding tasks like RAG, OCR, and NLP, enabling a responsive and high-quality user experience.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Intel OpenVINO Toolkit**  \n",
    "- **Purpose and Role**: OpenVINO (Open Visual Inference and Neural network Optimization) toolkit optimizes and accelerates deep learning models on Intel hardware, supporting model optimization, quantization, and hardware-specific acceleration.\n",
    "- **Benefits for the Project**: OpenVINO optimizes the LLaMA model and embedding pipeline for Intel GPUs, significantly improving inference times and reducing latency for instant user responses.\n",
    "- **Why It Matters**: OpenVINO ensures the assistant meets real-time performance while maintaining accuracy, enhancing overall user experience.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. OCR with PyTesseract**  \n",
    "- **Purpose and Role**: Optical Character Recognition (OCR) with PyTesseract extracts text from images, particularly effective for recognizing nutritional labels.\n",
    "- **Benefits for the Project**: PyTesseract processes various label fonts and formats, converting them into structured data crucial for the RAG assistant’s understanding of ingredients and nutritional content.\n",
    "- **Why It Matters**: OCR integration enables users to analyze food labels easily by snapping a photo, providing a seamless and user-friendly experience.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. LLaMA 3.1 8B Instruct**  \n",
    "- **Purpose and Role**: The LLaMA model generates natural-sounding, contextually accurate text, answering user questions and offering nutritional insights.\n",
    "- **Benefits for the Project**: As the assistant’s core, LLaMA provides contextual insights and suggestions, adding a human-like, interactive component.\n",
    "- **Why It Matters**: LLaMA’s NLP capabilities ensure that users receive detailed, accurate responses, making it both informative and easy to engage with.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. LangChain Integration**  \n",
    "- **Purpose and Role**: LangChain connects various AI system components, enabling smooth interactions between the language model, OCR, and other modules.\n",
    "- **Benefits for the Project**: LangChain facilitates a seamless experience where OCR-extracted data flows into the LLaMA model, allowing users to ask questions about nutritional content and get accurate answers instantly.\n",
    "- **Why It Matters**: LangChain simplifies backend architecture, allowing complex queries and interactions, ultimately enhancing usability.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 14px; font-style: italic;\">Created for the Intel Student Ambassador Fall Hackathon 2024 by</span> **Ilias Amchichou**, **Ishparsh Uprety**, <span style=\"font-weight: bold;\">and</span> **Visharad Kashyap**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1174a-91c5-44fa-a515-5e13f67c4383",
   "metadata": {},
   "source": [
    "## Install dependencies and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f077b32-5d36-44b0-9041-407e996283a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "from pip_helper import pip_install\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "pip_install(\"--pre\", \"-U\", \"openvino>=2024.2.0\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "pip_install( \"openvino-tokenizers[transformers]\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "pip_install(\n",
    "    \"--extra-index-url\",\n",
    "    \"https://download.pytorch.org/whl/cpu\",\n",
    "    \"git+https://github.com/huggingface/optimum-intel.git\",\n",
    "    \"nncf\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"gradio>=4.19\",\n",
    "    \"onnx<1.16.2\",\n",
    "    \"einops\",\n",
    "    \"transformers_stream_generator\",\n",
    "    \"tiktoken\",\n",
    "    \"transformers>=4.43.1\",\n",
    "    \"faiss-cpu\",\n",
    "    \"sentence_transformers\",\n",
    "    \"langchain>=0.2.0\",\n",
    "    \"langchain-community>=0.2.15\",\n",
    "    \"langchainhub\",\n",
    "    \"unstructured\",\n",
    "    \"scikit-learn\",\n",
    "    \"python-docx\",\n",
    "    \"pypdf\",\n",
    "    \"pytesseract\",\n",
    "    \"pillow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fab024-495a-400f-9ea0-c09d8195daa2",
   "metadata": {},
   "source": [
    "## Import LLM configuration file and your document(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "text_example_en_path = Path(\"text_example_en.pdf\")\n",
    "\n",
    "if not text_example_en_path.exists():\n",
    "    r = requests.get(url=text_example_en)\n",
    "    content = io.BytesIO(r.content)\n",
    "    with open(\"text_example_en.pdf\", \"wb\") as f:\n",
    "        f.write(content.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b57cfb-e727-43a5-b2c9-8f1b1ba72061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from transformers import (\n",
    "    TextIteratorStreamer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0320f-953d-4593-ae6d-3b3836f58765",
   "metadata": {},
   "source": [
    "## Select the language and the Large Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_config import (\n",
    "    SUPPORTED_EMBEDDING_MODELS,\n",
    "    SUPPORTED_RERANK_MODELS,\n",
    "    SUPPORTED_LLM_MODELS,\n",
    ")\n",
    "\n",
    "model_languages = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "model_language = widgets.Dropdown(\n",
    "    options=model_languages,\n",
    "    value=model_languages[0],\n",
    "    description=\"Model Language:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "model_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d1678-0e73-4f35-8af5-1a7d291c2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_ids = [model_id for model_id, model_config in SUPPORTED_LLM_MODELS[model_language.value].items() if model_config.get(\"rag_prompt_template\")]\n",
    "\n",
    "llm_model_id = widgets.Dropdown(\n",
    "    options=llm_model_ids,\n",
    "    value=llm_model_ids[-1],\n",
    "    description=\"Model:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "llm_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d1abb-45da-4dd9-a517-4b3b6d9f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_configuration = SUPPORTED_LLM_MODELS[model_language.value][llm_model_id.value]\n",
    "print(f\"Selected LLM model {llm_model_id.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81403d25-30a8-46c3-a824-be4e55fecb4d",
   "metadata": {},
   "source": [
    "## Prepare the model for LLM conversion and Weights Compression using Optimum-CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a38153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "prepare_int8_model = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "\n",
    "display(prepare_int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4531bbd67d8753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_awq = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Enable AWQ\",\n",
    "    disabled= prepare_int8_model.value,\n",
    ")\n",
    "display(enable_awq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model_id = llm_model_configuration[\"model_id\"]\n",
    "pt_model_name = llm_model_id.value.split(\"-\")[0]\n",
    "\n",
    "int8_model_dir = Path(llm_model_id.value) / \"INT8_compressed_weights\"\n",
    "\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remote_code = llm_model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int8\".format(pt_model_id)\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int8_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaebdf6-de2d-4ef7-b44f-ce1e8b477339",
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "for precision, compressed_weights in zip([8], [int8_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc08766-cc6d-4945-bc4a-e2fd2a14decd",
   "metadata": {},
   "source": [
    "## Select the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c28d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = list(SUPPORTED_EMBEDDING_MODELS[model_language.value])\n",
    "\n",
    "embedding_model_id = widgets.Dropdown(\n",
    "    options=embedding_model_id,\n",
    "    value=embedding_model_id[0],\n",
    "    description=\"Embedding Model:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "embedding_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_configuration = SUPPORTED_EMBEDDING_MODELS[model_language.value][embedding_model_id.value]\n",
    "print(f\"Selected {embedding_model_id.value} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd60408-58af-4ddf-b11a-4135fa0f6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_command_base = \"optimum-cli export openvino --model {} --task feature-extraction\".format(embedding_model_configuration[\"model_id\"])\n",
    "export_command = export_command_base + \" \" + str(embedding_model_id.value)\n",
    "\n",
    "if not Path(embedding_model_id.value).exists():\n",
    "    ! $export_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46c2a0-d53b-46fb-be3e-47ae2e156928",
   "metadata": {},
   "source": [
    "## Select the device to load the Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "embedding_device = device_widget()\n",
    "\n",
    "embedding_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab29b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Embedding model will be loaded to {embedding_device.value} device for text embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import optimize_bge_embedding\n",
    "\n",
    "USING_NPU = embedding_device.value == \"NPU\"\n",
    "\n",
    "npu_embedding_dir = embedding_model_id.value + \"-npu\"\n",
    "npu_embedding_path = Path(npu_embedding_dir) / \"openvino_model.xml\"\n",
    "if USING_NPU and not Path(npu_embedding_dir).exists():\n",
    "    shutil.copytree(embedding_model_id.value, npu_embedding_dir)\n",
    "    optimize_bge_embedding(Path(embedding_model_id.value) / \"openvino_model.xml\", npu_embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ebe0b-9754-4342-afbf-2fde1f3690cc",
   "metadata": {},
   "source": [
    "## Select the device to load the Large Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d044d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "llm_device = device_widget(\"GPU\")\n",
    "\n",
    "llm_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LLM model will be loaded to {llm_device.value} device for response generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe58008-24c4-47a4-9f63-92d99106d25e",
   "metadata": {},
   "source": [
    "## Loading the Hugging Face embedding model using OpenVINO through OpenVINOBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e8fd1-d4c1-4e33-b46e-7840e392f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenVINOBgeEmbeddings\n",
    "\n",
    "embedding_model_name = npu_embedding_dir if USING_NPU else embedding_model_id.value\n",
    "batch_size = 1 if USING_NPU else 4\n",
    "embedding_model_kwargs = {\"device\": embedding_device.value, \"compile\": False}\n",
    "encode_kwargs = {\n",
    "    \"mean_pooling\": embedding_model_configuration[\"mean_pooling\"],\n",
    "    \"normalize_embeddings\": embedding_model_configuration[\"normalize_embeddings\"],\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "\n",
    "embedding = OpenVINOBgeEmbeddings(\n",
    "    model_name_or_path=embedding_model_name,\n",
    "    model_kwargs=embedding_model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "if USING_NPU:\n",
    "    embedding.ov_model.reshape(1, 512)\n",
    "embedding.ov_model.compile()\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "embedding_result = embedding.embed_query(text)\n",
    "embedding_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b968f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = []\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82055dc9-0f42-4baf-977c-1370a9de5f72",
   "metadata": {},
   "source": [
    " ## Triggering OpenVINO as backend inference framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f708db-8de1-4efd-94b2-fcabc48d52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "\n",
    "\n",
    "\n",
    "model_dir = int8_model_dir\n",
    "\n",
    "ov_config = {hints.performance_mode(): hints.PerformanceMode.LATENCY, streams.num(): \"1\", props.cache_dir(): \"\"}\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=str(model_dir),\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\n",
    "        \"device\": llm_device.value,\n",
    "        \"ov_config\": ov_config,\n",
    "        \"trust_remote_code\": True,\n",
    "    },\n",
    "    pipeline_kwargs={\"max_new_tokens\": 2},\n",
    ")\n",
    "\n",
    "if llm.pipeline.tokenizer.eos_token_id:\n",
    "    llm.pipeline.tokenizer.pad_token_id = llm.pipeline.tokenizer.eos_token_id\n",
    "\n",
    "llm.invoke(\"2 + 2 =\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcedc62-7089-4d1a-ac11-0e010e8f1497",
   "metadata": {},
   "source": [
    "## Running QA over Document\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "- **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happen offline.\n",
    "\n",
    "- **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:\n",
    "\n",
    "**Indexing**\n",
    "\n",
    "1. `Load`: First we need to load our data. We’ll use DocumentLoaders for this.\n",
    "2. `Split`: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t in a model’s finite context window.\n",
    "3. `Store`: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "![Indexing pipeline](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/dfed2ba3-0c3a-4e0e-a2a7-01638730486a)\n",
    "\n",
    "**Retrieval and generation**\n",
    "\n",
    "1. `Retrieve`: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "2. `Generate`: A LLM produces an answer using a prompt that includes the question and the retrieved data.\n",
    "\n",
    "![Retrieval and generation pipeline](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/f0545ddc-c0cd-4569-8c86-9879fdab105a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownTextSplitter,\n",
    ")\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "\n",
    "\n",
    "TEXT_SPLITERS = {\n",
    "    \"Character\": CharacterTextSplitter,\n",
    "    \"RecursiveCharacter\": RecursiveCharacterTextSplitter,\n",
    "    \"Markdown\": MarkdownTextSplitter,\n",
    "}\n",
    "\n",
    "\n",
    "LOADERS = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "}\n",
    "\n",
    "text_example_path = \"text_example_en.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0908e5e9-4dcb-4fc8-8480-3cf70fd5e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from threading import Thread\n",
    "import gradio as gr\n",
    "\n",
    "stop_tokens = llm_model_configuration.get(\"stop_tokens\")\n",
    "rag_prompt_template = llm_model_configuration[\"rag_prompt_template\"]\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "if stop_tokens is not None:\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = llm.pipeline.tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "\n",
    "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "\n",
    "\n",
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    helper for loading a single document\n",
    "\n",
    "    Params:\n",
    "      file_path: document path\n",
    "    Returns:\n",
    "      documents loaded\n",
    "\n",
    "    \"\"\"\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADERS:\n",
    "        loader_class, loader_args = LOADERS[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"File does not exist '{ext}'\")\n",
    "\n",
    "\n",
    "def default_partial_text_processor(partial_text: str, new_text: str):\n",
    "    \"\"\"\n",
    "    helper for updating partially generated answer, used by default\n",
    "\n",
    "    Params:\n",
    "      partial_text: text buffer for storing previosly generated text\n",
    "      new_text: text update for the current step\n",
    "    Returns:\n",
    "      updated text string\n",
    "\n",
    "    \"\"\"\n",
    "    partial_text += new_text\n",
    "    return partial_text\n",
    "\n",
    "\n",
    "text_processor = llm_model_configuration.get(\"partial_text_processor\", default_partial_text_processor)\n",
    "\n",
    "\n",
    "def create_vectordb(\n",
    "    docs, spliter_name, chunk_size, chunk_overlap, vector_search_top_k, vector_rerank_top_n, run_rerank, search_method, score_threshold, progress=gr.Progress()\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize a vector database\n",
    "\n",
    "    Params:\n",
    "      doc: orignal documents provided by user\n",
    "      spliter_name: spliter method\n",
    "      chunk_size:  size of a single sentence chunk\n",
    "      chunk_overlap: overlap size between 2 chunks\n",
    "      vector_search_top_k: Vector search top k\n",
    "      vector_rerank_top_n: Search rerank top n\n",
    "      run_rerank: whether run reranker\n",
    "      search_method: top k search method\n",
    "      score_threshold: score threshold when selecting 'similarity_score_threshold' method\n",
    "\n",
    "    \"\"\"\n",
    "    global db\n",
    "    global retriever\n",
    "    global combine_docs_chain\n",
    "    global rag_chain\n",
    "\n",
    "    if vector_rerank_top_n > vector_search_top_k:\n",
    "        gr.Warning(\"Search top k must >= Rerank top n\")\n",
    "\n",
    "    documents = []\n",
    "    for doc in docs:\n",
    "        if type(doc) is not str:\n",
    "            doc = doc.name\n",
    "        documents.extend(load_single_document(doc))\n",
    "\n",
    "    text_splitter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    db = FAISS.from_documents(texts, embedding)\n",
    "    if search_method == \"similarity_score_threshold\":\n",
    "        search_kwargs = {\"k\": vector_search_top_k, \"score_threshold\": score_threshold}\n",
    "    else:\n",
    "        search_kwargs = {\"k\": vector_search_top_k}\n",
    "    retriever = db.as_retriever(search_kwargs=search_kwargs, search_type=search_method)\n",
    "    if run_rerank:\n",
    "        reranker.top_n = vector_rerank_top_n\n",
    "        retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=retriever)\n",
    "    prompt = PromptTemplate.from_template(rag_prompt_template)\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "    return \"Vector database is Ready\"\n",
    "\n",
    "\n",
    "def update_retriever(vector_search_top_k, vector_rerank_top_n, run_rerank, search_method, score_threshold):\n",
    "    \"\"\"\n",
    "    Update retriever\n",
    "\n",
    "    Params:\n",
    "      vector_search_top_k: Vector search top k\n",
    "      vector_rerank_top_n: Search rerank top n\n",
    "      run_rerank: whether run reranker\n",
    "      search_method: top k search method\n",
    "      score_threshold: score threshold when selecting 'similarity_score_threshold' method\n",
    "\n",
    "    \"\"\"\n",
    "    global db\n",
    "    global retriever\n",
    "    global combine_docs_chain\n",
    "    global rag_chain\n",
    "\n",
    "    if vector_rerank_top_n > vector_search_top_k:\n",
    "        gr.Warning(\"Search top k must >= Rerank top n\")\n",
    "\n",
    "    if search_method == \"similarity_score_threshold\":\n",
    "        search_kwargs = {\"k\": vector_search_top_k, \"score_threshold\": score_threshold}\n",
    "    else:\n",
    "        search_kwargs = {\"k\": vector_search_top_k}\n",
    "    retriever = db.as_retriever(search_kwargs=search_kwargs, search_type=search_method)\n",
    "    if run_rerank:\n",
    "        retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=retriever)\n",
    "        reranker.top_n = vector_rerank_top_n\n",
    "    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "    return \"Vector database is Ready\"\n",
    "\n",
    "\n",
    "def bot(history, temperature, top_p, top_k, repetition_penalty, hide_full_prompt, do_rag):\n",
    "    \"\"\"\n",
    "    callback function for running chatbot on submit button click\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
    "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
    "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
    "      top_k: parameter for control the range of tokens considered by the AI model based on their cumulative probability, selecting number of tokens with highest probability.\n",
    "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
    "      hide_full_prompt: whether to show searching results in prompt.\n",
    "      do_rag: whether do RAG when generating texts.\n",
    "\n",
    "    \"\"\"\n",
    "    streamer = TextIteratorStreamer(\n",
    "        llm.pipeline.tokenizer,\n",
    "        timeout=3600.0,\n",
    "        skip_prompt=hide_full_prompt,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    pipeline_kwargs = dict(\n",
    "        max_new_tokens=512,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0.0,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    if stop_tokens is not None:\n",
    "        pipeline_kwargs[\"stopping_criteria\"] = StoppingCriteriaList(stop_tokens)\n",
    "\n",
    "    llm.pipeline_kwargs = pipeline_kwargs\n",
    "    if do_rag:\n",
    "        retrieved_result = rag_chain.invoke({\"input\": history[-1][0]})\n",
    "\n",
    "        context_text = \" \".join([doc.page_content for doc in retrieved_result[\"documents\"]])\n",
    "\n",
    "        history.append([\"Retrieved Context\", context_text])\n",
    "\n",
    "        input_text = rag_prompt_template.format(input=history[-1][0], context=context_text)\n",
    "\n",
    "        t1 = Thread(target=llm.invoke, args=(input_text,))\n",
    "    else:\n",
    "        input_text = rag_prompt_template.format(input=history[-1][0], context=\"\")\n",
    "        t1 = Thread(target=llm.invoke, args=(input_text,))\n",
    "    t1.start()\n",
    "\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text = text_processor(partial_text, new_text)\n",
    "        history[-1][1] = partial_text\n",
    "        yield history\n",
    "\n",
    "\n",
    "def request_cancel():\n",
    "    llm.pipeline.model.request.cancel()\n",
    "\n",
    "\n",
    "create_vectordb(\n",
    "    [text_example_path],\n",
    "    \"RecursiveCharacter\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    vector_search_top_k=10,\n",
    "    vector_rerank_top_n=2,\n",
    "    run_rerank=True,\n",
    "    search_method=\"similarity_score_threshold\",\n",
    "    score_threshold=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0201ff3-e54c-42de-9723-b3fbdf469c47",
   "metadata": {},
   "source": [
    "## Creating a UI using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412ca3e-20ee-4929-b1d8-c211201d1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_helper import make_demo\n",
    "\n",
    "demo = make_demo(\n",
    "    load_doc_fn=create_vectordb,\n",
    "    run_fn=bot,\n",
    "    stop_fn=request_cancel,\n",
    "    update_retriever_fn=update_retriever,\n",
    "    model_name=llm_model_id.value,\n",
    ")\n",
    "\n",
    "try:\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b5a84-bebf-49b9-b2fa-5e788ed2cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/304aa048-f10c-41c6-bb31-6d2bfdf49cf5",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
